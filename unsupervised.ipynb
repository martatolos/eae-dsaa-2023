{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/martatolos/eae-dsaa-2025/blob/main/unsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbsqBHGhx56n"
   },
   "source": [
    "# Unsupervised Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Goal of the session:\n",
    ">\n",
    "> - At the end of this activity, you will understand the basics of unsupervised algorithms and how to apply them. Also see their limitations and how some approaches try to overcome them.\n",
    ">\n",
    "> Scope of the session\n",
    ">\n",
    "> - Understand the motivation and use cases of unsupervised algorithms.\n",
    "> - Prepare a dataset for unsupervised learning.\n",
    "> - Use KMeans clustering to group data points.\n",
    "> - Analyze how KMeans clustering works.\n",
    "> - Go through the limitations of KMeans clustering.\n",
    "> - See metrics and methodologies to try to evaluate whether clustering is effective.\n",
    "> - Use methods such as Spectral Clustering cluster data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Unsupervised learning: recap theory"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzcmuNsfx56p"
   },
   "source": [
    "**Describe in your own words: what is unsupervised learning?**\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Show solution</summary>\n",
    "  <ul>\n",
    "    <li>Unsupervised algorithms are those whose training data consists of a set of input variables $X$ without a target variable $Y$.</li>\n",
    "    <li>=> We have no labels for our data points</li>\n",
    " </ul>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU1zv8U6x56p"
   },
   "source": [
    "The main categories of unsupervised learning are:\n",
    "\n",
    "**Clustering**\n",
    "- Discovering groups with similar features within a dataset\n",
    "- Grouping data points into clusters based on similarity\n",
    "- Examples: K-Means, DBSCAN, Hierarchical Clustering\n",
    "\n",
    "**Dimensionality reduction**\n",
    "- Reducing the number of variables/features while preserving important information\n",
    "- This reduction will allow the visualization as well as a better knowledge about your data\n",
    "- Examples: PCA (Principal Component Analysis), t-SNE, UMAP\n",
    "\n",
    "**Anomaly Detection**\n",
    "- Identifying rare or unusual data points that differ significantly from the majority.\n",
    "- Examples: One-Class SVM, Isolation Forest, Autoencoders (unsupervised setup)\n",
    "\n",
    "**Association Rule Learning**\n",
    "- Discovering interesting relationships (rules) between variables in large datasets\n",
    "- Example: Apriori, Eclat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "othWbSYHx56p"
   },
   "source": "#### Applications"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOIh6P0px56q"
   },
   "source": "* **Customer segmentation**: the market is divided into smaller segments of buyers who have different needs, characteristics and behaviors to apply different strategies."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKv5Mcilx56q"
   },
   "source": [
    "* **Fraud detection**: identify which transactions can be considered false pretenses. It is about finding anomalous behaviours which are not related to the general behaviour of the rest of the population.\n",
    "\n",
    "> Kaggle example: http://archive.ics.uci.edu/ml/datasets/statlog+(australian+credit+approval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz8faeEyx56r"
   },
   "source": [
    "* **Face detection using PCA**: principal component analysis is used to reduce the number of variables. The data is compressed in such a way that the main characteristics are preserved. In the case of an image where a face appears, we know that not all the pixels represent the main features of the face. Using PCA, we extract the main ones which define a face and reduce dimensions.  \n",
    "> PCA example from scratch to detect faces: [implementation example](https://medium.com/@reubenrochesingh/building-face-detector-using-principal-component-analysis-pca-from-scratch-in-python-1e57369b8fc5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc_I375vx56r"
   },
   "source": "### Unsupervised learning in Python"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**\n",
    "\n",
    "- ``numpy`` 2.0.2\n",
    "- ``nbformat``\n",
    "- ``pandas`` 2.2.2\n",
    "- ``plotly`` 5.24.1\n",
    "- ``scikit-learn`` 1.6.1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install numpy==2.0.2 nbformat pandas==2.2.2 plotly==5.24.1 scikit-learn==1.6.1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Imports**"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "from sklearn.metrics import pairwise_distances_argmin, silhouette_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### K-means clustering"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2yZVbGNx56r"
   },
   "source": "*K-means* is an unsupervised algorithm from *clustering* category. Its purpose is to partition a set of $n$ observations / objects (e.g., images or attributes of customers) into $k$ groups where each observation belongs to the group whose **mean value is the closest**."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtKp0Xxzx56s"
   },
   "source": [
    "**Data generation**\n",
    "\n",
    "First, we'll generate example data for illustrating the algorithm. For this, we create a two dimensional example using one of the `scikit-learn` component called `make\\_blobs` to generate clusters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9DdRE-xZx56s"
   },
   "source": [
    "# Generate 300 data points belonging to 3 different clusters\n",
    "n_clusters = 3\n",
    "X, y = make_blobs(n_samples=300, centers=n_clusters, cluster_std=0.50, random_state=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the generated data\n",
    "fig = px.scatter(x=X[:, 0], y=X[:, 1], size_max=15)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can inspect the documentation of the component using the `help()` function or refer to the [website](https://scikit-learn.org/0.15/modules/generated/sklearn.datasets.make_blobs.html)."
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vISlyqvDx56t"
   },
   "source": [
    "# Print the documentation\n",
    "help(make_blobs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vND8cd4wx56u"
   },
   "source": "We can see that the number of groups are 3. The ${K-means}$ algorithm should detect automatically to which group each data point has to be assigned."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Applying K-Means**"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKSmTIyKx56u"
   },
   "source": "For running the algorithm we have instantiate an instance of `KMeans` class and run the `fit` method:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BYnYunOQx56v"
   },
   "source": [
    "# Create kmeans instance and fit the model to the data\n",
    "kmeans = KMeans(n_clusters=n_clusters)  # Number of groups are pre-defined\n",
    "kmeans.fit(X)  # Training"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUGcYGEF5Sgg"
   },
   "source": "We can use the model to predict the cluster / group to which a data point belongs to"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "udhg5edKx56v"
   },
   "source": [
    "# Predict the cluster for data point\n",
    "y_predicted = kmeans.predict(X)  # Prediction\n",
    "y_predicted"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjFb28EKx56v"
   },
   "source": "Each of the 300 points has been associated with one of the three previously set groups. We could apply the same to new data points to predict the group to which they belong."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "in-wMYb65h5x"
   },
   "source": [
    "**Visualize the result k-means works**\n",
    "\n",
    "For representing clusters, K-Means stores the center centroids (i.e., the mean vector of all points assigned to the cluster)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ynCuVHDUx56w"
   },
   "source": [
    "centers = kmeans.cluster_centers_\n",
    "print(centers)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9bLQwKDx56w"
   },
   "source": "We plot the points with the colour associated to their specific group. We also plot the centroids groups which are defined as the minimum mean distance of each set."
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pQN3GBlRx56x"
   },
   "source": [
    "# Create a figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add data points colored by their cluster labels\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\"color\": y_predicted, \"colorscale\": \"viridis\", \"size\": 10},\n",
    "        name=\"Data Points\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add cluster centers\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=centers[:, 0],\n",
    "        y=centers[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\"color\": \"red\", \"size\": 12, \"symbol\": \"x\"},\n",
    "        name=\"Cluster Centers\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title=\"K-means Clustering Results\", xaxis_title=\"Feature 1\", yaxis_title=\"Feature 2\")\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### How K-Means work"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TISzfu1gx56x"
   },
   "source": [
    "**Describe in your own words: how does K-Means work?**\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Show solution</summary>\n",
    "    This method is based on the **Expectation-Maximization algorithm** and the approach consists of the following steps:\n",
    "\n",
    "    1. Initial estimation of the centroids.\n",
    "    2. *Expectation step*: Assign the points to the closest cluster.\n",
    "    3. *Maximization step*: Set the centroids based on the new computed mean.\n",
    "    4. Go back to step 2 if the centroids have changed. Otherwise, stop.\n",
    "\n",
    "    When the centroids are not changing, the algorithm has converged.\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Py82o3I1x56y"
   },
   "source": [
    "See the following code, the first function implements the same process we run with ``scikit-learn`` and the second allows to deep dive into the algorithm to see how it works though visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UAKRDV2Sx56y"
   },
   "source": [
    "def find_clusters(\n",
    "    X: np.ndarray[np.float64],\n",
    "    centers: np.ndarray[np.float64],\n",
    "    max_iters: int,\n",
    ") -> tuple[np.ndarray[np.float64], np.ndarray[np.float64], int]:\n",
    "    \"\"\"Find the clusters within a dataset.\n",
    "\n",
    "    :param X: Dataset with samples to be clustered.\n",
    "    :param centers: Initial centers of the clusters.\n",
    "    :param max_iters: Maximum number of iterations.\n",
    "    :return: Tuple with the centers and labels for each iteration and the number of iterations.\n",
    "    \"\"\"\n",
    "    # Initial parameters\n",
    "    iters = 0\n",
    "    n_clusters = len(centers)  # Number of clusters\n",
    "    centers_iters = []  # Save centers for each iteration\n",
    "    labels_iters = []  # Save assignments for each iteration\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        # Compute the Euclidean distance from each point to each centroid\n",
    "        distances = np.linalg.norm(X[:, None] - centers, axis=2)\n",
    "\n",
    "        # Assign each point to the closet the center based on the computed distances\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "\n",
    "        # Save results\n",
    "        centers_iters.append(centers)\n",
    "        labels_iters.append(labels)\n",
    "\n",
    "        # Reallocate the centroids\n",
    "        new_centers = np.array([X[labels == i].mean(0) for i in range(n_clusters)])\n",
    "\n",
    "        # Check convergence\n",
    "        # In this case we're forcing the function to reach the same center as the cluster\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "\n",
    "        centers = new_centers\n",
    "        iters += 1\n",
    "\n",
    "    # The output lists are converted to numpy arrays.\n",
    "    return np.array(centers_iters, dtype=np.float64), np.array(labels_iters, dtype=np.float64), iters"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g__fyY8lx56z"
   },
   "source": [
    "def visualize_kmeans_process(centers_iters: np.ndarray, labels_iters: np.ndarray, n_clusters: int, iters: int) -> None:\n",
    "    \"\"\"\n",
    "    Visualize the k-means process for each iteration using Plotly.\n",
    "\n",
    "    :param centers_iters: Centers for each iteration\n",
    "    :param labels_iters: Assignments for each iteration\n",
    "    :param n_clusters: Number of clusters\n",
    "    :param iters: Number of iterations until convergence.\n",
    "    \"\"\"\n",
    "    n_plots = iters + 1\n",
    "    n_cols = min(3, n_plots)\n",
    "    n_rows = int(np.ceil(n_plots / n_cols))\n",
    "\n",
    "    # Determine figure width and height based on number of columns and rows\n",
    "    width = n_cols * 600\n",
    "    height = n_rows * 400\n",
    "\n",
    "    # Create subplots with computed number of rows and columns\n",
    "    fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=[f\"Iteration {i}\" for i in range(n_plots)])\n",
    "\n",
    "    for i in range(n_plots):\n",
    "        row = i // n_cols + 1\n",
    "        col = i % n_cols + 1\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X[:, 0],\n",
    "                y=X[:, 1],\n",
    "                mode=\"markers\",\n",
    "                marker={\"color\": labels_iters[i], \"colorscale\": \"viridis\", \"size\": 8},\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "        for cluster in range(n_clusters):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[centers_iters[i][cluster][0]],\n",
    "                    y=[centers_iters[i][cluster][1]],\n",
    "                    mode=\"markers\",\n",
    "                    marker={\"color\": \"red\", \"size\": 10, \"symbol\": \"x\"},\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(title=\"K-means Clustering Process\", width=width, height=height)\n",
    "    fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "M4w1wJhoBtKn"
   },
   "source": [
    "centers = np.array([[1, 1], [2, 3], [2, 1]])\n",
    "#centers = np.array([[1, 1], [1, 3], [2, 1]])\n",
    "\n",
    "centers_iters, labels_iters, iters = find_clusters(X, centers, 10)\n",
    "\n",
    "n_clusters = len(centers)\n",
    "visualize_kmeans_process(centers_iters, labels_iters, n_clusters, iters)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZntE_wFx560"
   },
   "source": "The first graph shows an initial cluster assignment that is not the desired one because of the random centroids used. However, the centroids are getting closer to their corresponding groups until the solution coverges. **It happens when the distance of the points to the closest centroid does not produce new assigments**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Exercise: Run the algorithm using different starting points. Investigate the obtained results and the algorithm behaviour (e.g., after how many iterations does the solution converges)**"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXN5j4-ax560"
   },
   "source": "**Does the result depend on the initial centroids?**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vgMQWjKax561"
   },
   "source": [
    "centers = np.array([[2, 0], [3, 1], [2, 1]])\n",
    "\n",
    "centers_iters, labels_iters, iters = find_clusters(X, centers, 20)\n",
    "\n",
    "n_clusters = len(centers)\n",
    "visualize_kmeans_process(centers_iters, labels_iters, n_clusters, iters)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gsD7Kb24x561"
   },
   "source": [
    "centers = np.array([[1, 1], [2, 3]])\n",
    "\n",
    "centers_iters, labels_iters, iters = find_clusters(X, centers)\n",
    "\n",
    "n_clusters = len(centers)\n",
    "visualize_kmeans_process(centers_iters, labels_iters, n_clusters, iters)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4640GP1x569"
   },
   "source": [
    "### Limitations of K-means\n",
    "\n",
    "**Limitation 1: Number of clusters**\n",
    "\n",
    "One of the most important limitations is that $K-means$ needs the number of groups as an argument. How are we going to know a priori the number of groups if we want to use this method to figure it out?\n",
    "\n",
    "What happen if we had chosen a different number of clusters?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8NBqNodIx56-"
   },
   "source": [
    "kmeans = KMeans(n_clusters=5)  # Set number of clusters\n",
    "kmeans.fit(X)  # Training\n",
    "y_kmeans = kmeans.predict(X)  # Prediction\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\"color\": y_kmeans, \"colorscale\": \"Viridis\", \"size\": 10},\n",
    "        name=\"Data Points\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=centers[:, 0],\n",
    "        y=centers[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\"color\": \"red\", \"symbol\": \"x\", \"size\": 12},\n",
    "        name=\"Cluster Centers\",\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem, we can execute multiple $K-means$ with different number of groups (i.e., with different values for `k`) and choose the one which meets a certain criteria. There are several criteria that allow us to measure \"how well\" the clusters have achieved. The two most famous criteria are the Elbow and the Silhouette methods.\n",
    "\n",
    "_**Elbow Method**_\n",
    "- Run $K-Means$ for a range of $k$ values (e.g., 1 to 10).\n",
    "- For each k, compute the Within-Cluster Sum of Squares (WCSS) ‚Äî the sum of squared distances between points and their cluster centroids.\n",
    "- Plot `k` vs. WCSS.\n",
    "- The ‚Äúelbow‚Äù point in the graph (where WCSS starts to decrease more slowly) suggests the best value for k.\n",
    "\n",
    "Let's inspect the Elbow diagram for the given example."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Store WCSS for different values of k\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "wcss = []\n",
    "K_range = range(1, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)  # inertia_ is the WCSS\n",
    "\n",
    "# Plot Elbow Graph\n",
    "plt.plot(K_range, wcss, marker='o')\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"WCSS (Inertia)\")\n",
    "plt.xticks(K_range)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "IhJNrP6Rx56-"
   },
   "cell_type": "markdown",
   "source": [
    "_**Silhouette**_\n",
    "- Measures how similar an object is to its own cluster compared to other clusters.\n",
    "- It provides a combined measure of _cohesion_ and _separation_\n",
    "- _Cohesion_: How close each point in a cluster is to other points in the same cluster\n",
    "- _Separation_: How far apart a point is from points in other clusters\n",
    "\n",
    "How it works:\n",
    "- For each data point $i$:\n",
    "1. $a(i)$: average distance from i to all other points in the same cluster (intra-cluster distance).\n",
    "2. $b(i)$: lowest average distance from i to all points in any other cluster (nearest-cluster distance).\n",
    "3. Then compute the silhouette score for point by: $s(i) = b(i) - a(i) / max(a(i), b(i))$\n",
    "> $ùë†(ùëñ) ‚âà 1$: point is well clustered. <br />\n",
    "> $ùë†(ùëñ) ‚âà 0$: point lies between clusters. <br />\n",
    "> $ùë†(ùëñ) < 0:$ point might be in the wrong cluster.\n",
    "4. The overall silhouette score is the average of all $s(i)$ values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eAziFhjCx56-"
   },
   "source": [
    "scores = []\n",
    "groups = np.arange(2, 11)  # 2, 3, 4, ..., 8, 9, 10\n",
    "\n",
    "for k in groups:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10).fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    scores.append(silhouette_score(X, labels, metric=\"euclidean\"))\n",
    "\n",
    "# Create a figure using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add a line trace for silhouette scores\n",
    "fig.add_trace(go.Scatter(x=groups, y=scores, mode=\"lines+markers\"))\n",
    "fig.update_layout(\n",
    "    title=\"Silhouette Scores for Different Numbers of Clusters\",\n",
    "    xaxis_title=\"Number of Clusters\",\n",
    "    yaxis_title=\"Silhouette Score\",\n",
    "    xaxis={\"tickmode\": \"linear\"},\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In general, the following guideline help to interpret silhouette scores:\n",
    "\n",
    "| Score Range | Interpretation                            |\n",
    "| ----------- | ----------------------------------------- |\n",
    "| 0.71 ‚Äì 1.00 | Strong structure, clear clusters          |\n",
    "| 0.51 ‚Äì 0.70 | Reasonable structure                      |\n",
    "| 0.26 ‚Äì 0.50 | Weak structure, overlapping clusters      |\n",
    "| < 0.25      | No substantial structure, poor clustering |\n",
    "\n",
    "The silhouette score is often favoured since:\n",
    "- Works with any distance metric (e.g., Euclidean, Manhattan).\n",
    "- Helps choose the optimal number of clusters (k) programmatically by comparing average silhouette scores for different values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y33JKU8lx56_"
   },
   "source": [
    "**Limitation 2: Linear separation**\n",
    "\n",
    "The fundamental model assumptions of k-means (points will be closer to their own cluster center than to others) means that the algorithm will often be ineffective if the clusters have complicated geometries. In particular, the boundaries between k-means clusters will always be linear, which means that it will fail for more complicated boundaries. Consider the following data, along with the cluster labels found by the typical k-means approach:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ph0noo2Vx57A"
   },
   "source": [
    "X, y = make_circles(n_samples=400, factor=0.3, noise=0.05)\n",
    "fig = px.scatter(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    color=y.astype(str),\n",
    "    title=\"Circles Dataset - Instance Classes\",\n",
    "    labels={\"x\": \"Feature 1\", \"y\": \"Feature 2\", \"color\": \"Class\"},\n",
    ")\n",
    "fig.update_traces(marker={\"size\": 10})\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5xqhKMlx57A"
   },
   "source": "**Question: Do the groups above have a linear separation?**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v1MsSUAxx57B"
   },
   "source": [
    "kmeans = KMeans(n_clusters=2).fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bR3RMhndx57B"
   },
   "source": [
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add data points colored by their cluster labels\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\"color\": y_kmeans, \"colorscale\": \"viridis\", \"size\": 10},\n",
    "        name=\"Data Points\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add cluster centers\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=centers[:, 0],\n",
    "        y=centers[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\"color\": \"red\", \"size\": 12, \"symbol\": \"x\"},\n",
    "        name=\"Cluster Centers\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title=\"K-means Clustering Results\", xaxis_title=\"Feature 1\", yaxis_title=\"Feature 2\")\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One solution: Spectral clustering**\n",
    "\n",
    "In order to solve this, we can use a kernel transformation to project the data into a higher dimension where a linear separation is possible. We might imagine using the same trick to allow k-means to discover non-linear boundaries. One version of this kernelized k-means is implemented in `scikit-learn` within the `SpectralClustering` estimator. It uses the graph of nearest neighbors to compute a higher-dimensional representation of the data, and then assigns labels using a k-means algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HBFYl0ZAx57C"
   },
   "source": [
    "model = SpectralClustering(n_clusters=2, affinity=\"nearest_neighbors\", assign_labels=\"kmeans\")\n",
    "labels = model.fit_predict(X)\n",
    "\n",
    "# Create a figure using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add data points colored by their cluster labels\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\"color\": labels, \"colorscale\": \"viridis\", \"size\": 10},\n",
    "        name=\"Data Points\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title=\"Spectral Clustering Results\", xaxis_title=\"Feature 1\", yaxis_title=\"Feature 2\")\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YElB5Ghzx57C"
   },
   "source": [
    "**In real cases**, it's hard to check if your clusters have a linear separation because of the number of dimensions. The approach will be to try different models and see how it works based on your requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0YKDe1zx57D"
   },
   "source": [
    "**Limitation 3: Categorical Variables**\n",
    "\n",
    "The standard k-means algorithm isn't directly applicable to categorical data, for various reasons. The sample space for categorical data is discrete, and doesn't have a natural origin. A Euclidean distance function on such a space isn't really meaningful.\n",
    "\n",
    "There's a variation of k-means known as k-modes, introduced in this [paper by Zhexue Huang](http://www.cs.ust.hk/~qyang/Teaching/537/Papers/huang98extensions.pdf), which is suitable for categorical data.\n",
    "An example how this can be implemented in python can be found here:\n",
    "- https://towardsdatascience.com/the-k-prototype-as-clustering-algorithm-for-mixed-data-type-categorical-and-numerical-fe7c50538ebb"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dimensionality reduction\n",
    "\n",
    "Dimensionality reduction is the process of reducing the number of input features (dimensions) in a dataset while preserving as much relevant information as possible. It's especially useful when working with high-dimensional data, such as images, text, or sensor data.\n",
    "\n",
    "**Why is / can dimensionality reduction be important?**\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Show solution</summary>\n",
    "\n",
    "High-dimensional data can cause several issues:\n",
    "  <ul>\n",
    "    <li>Computational inefficiency: More features mean longer training times.</li>\n",
    "    <li>Curse of dimensionality: In high dimensions, data becomes sparse and less meaningful.</li>\n",
    "    <li>Overfitting: Models may learn noise instead of signal.</li>\n",
    "    <li>Redundancy: Some features may be highly correlated and not provide additional information.</li>\n",
    "   </ul>\n",
    "\n",
    "Reducing dimensions helps to:\n",
    "    <ul>\n",
    "        <li>Speed up machine learning algorithms.</li>\n",
    "        <li>Improve model performance and generalization.</li>\n",
    "        <li>Enable visualization in 2D or 3D.</li>\n",
    "    </ul>\n",
    "</details>\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Types of dimensionality reduction**\n",
    "\n",
    "1. Feature Selection\n",
    "- Select a subset of the original features.\n",
    "- Examples: removing low-variance features, recursive feature elimination.\n",
    "\n",
    "2. Feature Extraction (Transformation)\n",
    "- Create new features that are combinations of original ones\n",
    "- Examples: PCA (Principal Component Analysis) ‚Äì linear combinations preserving variance, t-SNE / UMAP ‚Äì nonlinear methods for visualization"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Principal Component Analysis (PCA)**\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to simplify large datasets by transforming them into a smaller set of variables called principal components, while preserving as much of the original data's variability as possible.\n",
    "\n",
    "_**How does it work (sketch)**_\n",
    "1. Center the data: subtract the mean of each feature to center the data around the origin.\n",
    "2. Compute covariance matrix: shows how features vary with respect to each other.\n",
    "3. Eigen decomposition: find eigenvectors and eigenvalues of the covariance matrix.\n",
    "- Eigenvectors = directions of the new feature space (principal components).\n",
    "- Eigenvalues = amount of variance carried in each direction.\n",
    "4. Sort components: by eigenvalue (variance explained).\n",
    "5. Project the data: Onto the top $k$ components to reduce dimensions.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**PCA in Python**\n",
    "\n",
    "The following example shows how to apply PCA on the MNIST dataset.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST data (70,000 images of 28x28 digits)\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X, y = X / 255.0, y.astype(int)  # Normalize pixel values\n",
    "\n",
    "X = X[:2000]\n",
    "y = y[:2000]\n",
    "\n",
    "# Applying pca\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10')\n",
    "plt.colorbar(scatter, ticks=range(10))\n",
    "plt.title(\"K-Means Clusters on MNIST (2D PCA)\")\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
