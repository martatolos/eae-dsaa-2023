{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/martatolos/eae-dsaa-2025/blob/main/unsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbsqBHGhx56n"
   },
   "source": [
    "# Unsupervised Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Goal of the session:\n",
    ">\n",
    "> - At the end of this activity, you will understand the basics of unsupervised algorithms and how to apply them. Also see their limitations and how some approaches try to overcome them.\n",
    ">\n",
    "> Scope of the session\n",
    ">\n",
    "> - Understand the motivation and use cases of unsupervised algorithms.\n",
    "> - Prepare a dataset for unsupervised learning.\n",
    "> - Use KMeans clustering to group data points.\n",
    "> - Analyze how KMeans clustering works.\n",
    "> - Go through the limitations of KMeans clustering.\n",
    "> - See metrics and methodologies to try to evaluate whether clustering is effective.\n",
    "> - Use methods such as Spectral Clustering cluster data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzcmuNsfx56p"
   },
   "source": [
    "Unsupervised algorithms are those whose training data consists of a set of input variables $X$ without a target variable $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU1zv8U6x56p"
   },
   "source": [
    "There are two categories:\n",
    "\n",
    "* **Clustering**: discover groups with similar features within a dataset.\n",
    "\n",
    "* **Dimensionality reduction**: reduce a dataset with a high number of dimensions to two or three ones. This reduction will allow the visualization as well as a better knowledge about your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "othWbSYHx56p"
   },
   "source": [
    "### Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOIh6P0px56q"
   },
   "source": [
    "* **Customer segmentation**: the market is divided into smaller segments of buyers who have different needs, characteristics and behaviors to apply different strategies. **Note:** You could apply a customer segmentation using the CRM exercise based on $age$, $GDP$, $gender$, $ first$ $purchase$, $visits$, .etc.\n",
    "\n",
    "![](https://i.pinimg.com/originals/d7/2f/7b/d72f7bde33d814881a5d058212228514.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKv5Mcilx56q"
   },
   "source": [
    "* **Fraud detection**: identify which transactions can be considered false pretenses. It is about finding anomalous behaviours which are not related to the general behaviour of the rest of the population.\n",
    "\n",
    "Kaggle example:\n",
    "http://archive.ics.uci.edu/ml/datasets/statlog+(australian+credit+approval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz8faeEyx56r"
   },
   "source": [
    "* **Face detection using PCA**: principal component analysis is used to reduce the number of variables. The data is compressed in such a way that the main characteristics are preserved. In the case of an image where a face appears, we know that not all the pixels represent the main features of the face. Using PCA, we extract the main ones which define a face and reduce dimensions.  \n",
    "\n",
    "PCA example from scratch to detect faces:\n",
    "\n",
    "https://medium.com/@reubenrochesingh/building-face-detector-using-principal-component-analysis-pca-from-scratch-in-python-1e57369b8fc5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc_I375vx56r"
   },
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "- ``numpy`` 2.0.2\n",
    "- ``nbformat``\n",
    "- ``pandas`` 2.2.2\n",
    "- ``plotly`` 5.24.1\n",
    "- ``scikit-learn`` 1.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy==2.0.2 nbformat pandas==2.2.2 plotly==5.24.1 scikit-learn==1.6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "from sklearn.metrics import pairwise_distances_argmin, silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2yZVbGNx56r"
   },
   "source": [
    "*K-means* technique is an unsupervised algorithm from *Clustering* category.\n",
    "\n",
    "Its purpose is to partition a set of $n$ observations into $k$ groups where each observation belongs to the group whose **mean value is the closest**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtKp0Xxzx56s"
   },
   "source": [
    "We create a two dimensional example using one of the $sklearn$ library component called $make\\_blobs$ to generate clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DdRE-xZx56s"
   },
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "X, y = make_blobs(n_samples=300, centers=n_clusters, cluster_std=0.50, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=X[:, 0], y=X[:, 1], size_max=15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vISlyqvDx56t"
   },
   "outputs": [],
   "source": [
    "help(make_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vND8cd4wx56u"
   },
   "source": [
    "We can see that the number of groups are 3. The ${K-means}$ algorithm should detect automatically to which group each data point has to be assigned:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKSmTIyKx56u"
   },
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYnYunOQx56v"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters)  # Number of groups are pre-defined\n",
    "kmeans.fit(X)  # Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUGcYGEF5Sgg"
   },
   "source": [
    "### Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udhg5edKx56v"
   },
   "outputs": [],
   "source": [
    "y_predicted = kmeans.predict(X)  # Prediction\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjFb28EKx56v"
   },
   "source": [
    "#### What is happening\n",
    "\n",
    "Each of the 300 points has been associated with one of the three previously set groups. We could apply the same to new data points to predict the group to which they belong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "in-wMYb65h5x"
   },
   "source": [
    "### Visualize how k-means works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPHHgYO-x56w"
   },
   "source": [
    "#### Figure out the center positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynCuVHDUx56w"
   },
   "outputs": [],
   "source": [
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9bLQwKDx56w"
   },
   "source": [
    "We represent the points with the colour associated to their specific group. We also plot the centroids groups which are defined as the minimum mean distance of each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQN3GBlRx56x"
   },
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add data points colored by their cluster labels\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\"color\": y_predicted, \"colorscale\": \"viridis\", \"size\": 10},\n",
    "        name=\"Data Points\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add cluster centers\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=centers[:, 0],\n",
    "        y=centers[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\"color\": \"red\", \"size\": 12, \"symbol\": \"x\"},\n",
    "        name=\"Cluster Centers\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title=\"K-means Clustering Results\", xaxis_title=\"Feature 1\", yaxis_title=\"Feature 2\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TISzfu1gx56x"
   },
   "source": [
    "$K-means$ algorithm assigns the points to the clusters as you would have done.\n",
    "\n",
    "The entire point is knowing how it works. The good news is this methodology is very simple and we could implement it by ourselves. This method is based on the **Expectation-Maximization algorithm** and the approach consists on:\n",
    "\n",
    "1. Initial estimation of the centroids.\n",
    "2. *Expectation step*: Assign the points to the closest cluster.\n",
    "3. *Maximization step*: Set the centroids based on the new computed mean.\n",
    "4. Go back if the centroids have changed. Otherwise, stop.\n",
    "\n",
    "When the centroids are not changing, the algorithm has converged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Py82o3I1x56y"
   },
   "source": [
    "See the following code, the first function implements the same process we run with ``scikit-learn`` and the second allows to deep dive into the algorithm to see how it works though visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAKRDV2Sx56y"
   },
   "outputs": [],
   "source": [
    "def find_clusters(\n",
    "    X: np.ndarray[np.float64], centers: np.ndarray[np.float64]\n",
    ") -> tuple[np.ndarray[np.float64], np.ndarray[np.float64], int]:\n",
    "    \"\"\"Find the clusters within a dataset.\n",
    "\n",
    "    :param iterable X: Dataset with samples to be clustered.\n",
    "    :param iterable centers: Initial centers of the clusters.\n",
    "    :return: Tuple with the centers and labels for each iteration and the number of iterations.\n",
    "    \"\"\"\n",
    "    # Initial parameters\n",
    "    iters = 0\n",
    "    n_clusters = len(centers)  # Number of clusters\n",
    "    centers_iters = []  # Save centers for each iteration\n",
    "    labels_iters = []  # Save assignments for each iteration\n",
    "\n",
    "    while True:\n",
    "        # Assign the points to the closest group\n",
    "        labels = pairwise_distances_argmin(X, centers)\n",
    "\n",
    "        # Save results\n",
    "        centers_iters.append(centers)\n",
    "        labels_iters.append(labels)\n",
    "\n",
    "        # Reallocate the centroids\n",
    "        new_centers = np.array([X[labels == i].mean(0) for i in range(n_clusters)])\n",
    "\n",
    "        # Check convergence\n",
    "        # In this case we're forcing the function to reach the same center as the cluster\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "\n",
    "        centers = new_centers\n",
    "        iters += 1\n",
    "\n",
    "    # The output lists are converted to numpy arrays.\n",
    "    return np.array(centers_iters, dtype=np.float64), np.array(labels_iters, dtype=np.float64), iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g__fyY8lx56z"
   },
   "outputs": [],
   "source": [
    "def visualize_kmeans_process(centers_iters: np.ndarray, labels_iters: np.ndarray, n_clusters: int, iters: int) -> None:\n",
    "    \"\"\"\n",
    "    Visualize the k-means process for each iteration using Plotly.\n",
    "\n",
    "    :param centers_iters: Centers for each iteration\n",
    "    :param labels_iters: Assignments for each iteration\n",
    "    :param n_clusters: Number of clusters\n",
    "    :param iters: Number of iterations until convergence.\n",
    "    \"\"\"\n",
    "    n_plots = iters + 1\n",
    "    n_cols = min(3, n_plots)\n",
    "    n_rows = int(np.ceil(n_plots / n_cols))\n",
    "\n",
    "    # Determine figure width and height based on number of columns and rows\n",
    "    width = n_cols * 600\n",
    "    height = n_rows * 400\n",
    "\n",
    "    # Create subplots with computed number of rows and columns\n",
    "    fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=[f\"Iteration {i}\" for i in range(n_plots)])\n",
    "\n",
    "    for i in range(n_plots):\n",
    "        row = i // n_cols + 1\n",
    "        col = i % n_cols + 1\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X[:, 0],\n",
    "                y=X[:, 1],\n",
    "                mode=\"markers\",\n",
    "                marker={\"color\": labels_iters[i], \"colorscale\": \"viridis\", \"size\": 8},\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "        for cluster in range(n_clusters):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[centers_iters[i][cluster][0]],\n",
    "                    y=[centers_iters[i][cluster][1]],\n",
    "                    mode=\"markers\",\n",
    "                    marker={\"color\": \"red\", \"size\": 10, \"symbol\": \"x\"},\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(title=\"K-means Clustering Process\", width=width, height=height)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4w1wJhoBtKn"
   },
   "outputs": [],
   "source": [
    "centers = np.array([[1, 1], [2, 3], [2, 1]])\n",
    "# centers = np.array([[1, 1], [1, 3], [2, 1]])\n",
    "\n",
    "centers_iters, labels_iters, iters = find_clusters(X, centers)\n",
    "\n",
    "n_clusters = len(centers)\n",
    "visualize_kmeans_process(centers_iters, labels_iters, n_clusters, iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3tujY8Px560"
   },
   "outputs": [],
   "source": [
    "help(kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZntE_wFx560"
   },
   "source": [
    "The first graph shows an initial cluster assignment that is not the desired one because of the random centroids used.\n",
    "\n",
    "However, the centroids are getting closer to their corresponding groups until the solution coverges. **It happens when the distance of the points to the closest centroid does not produce new assigments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXN5j4-ax560"
   },
   "source": [
    "### Does the result depend on the initial centroids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgMQWjKax561"
   },
   "outputs": [],
   "source": [
    "centers = np.array([[2, 0], [3, 1], [2, 1]])\n",
    "\n",
    "centers_iters, labels_iters, iters = find_clusters(X, centers)\n",
    "\n",
    "n_clusters = len(centers)\n",
    "visualize_kmeans_process(centers_iters, labels_iters, n_clusters, iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAtg6ql-x561"
   },
   "source": [
    "#### Using only two groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsD7Kb24x561"
   },
   "outputs": [],
   "source": [
    "centers = np.array([[1, 1], [2, 3]])\n",
    "\n",
    "centers_iters, labels_iters, iters = find_clusters(X, centers)\n",
    "\n",
    "n_clusters = len(centers)\n",
    "visualize_kmeans_process(centers_iters, labels_iters, n_clusters, iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4640GP1x569"
   },
   "source": [
    "### Limitations of K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOC6zSv4x569"
   },
   "source": [
    "#### Number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJT-SYpJx569"
   },
   "source": [
    "One of the most important limitations is that $K-means$ needs the number of groups as an argument. How are we going to know a priori the number of groups if we want to use this method to figure it out?\n",
    "\n",
    "What happen if we had chosen a different number of clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NBqNodIx56-"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5)  # Set number of clusters\n",
    "kmeans.fit(X)  # Training\n",
    "y_kmeans = kmeans.predict(X)  # Prediction\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\"color\": y_kmeans, \"colorscale\": \"Viridis\", \"size\": 10},\n",
    "        name=\"Data Points\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=centers[:, 0],\n",
    "        y=centers[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\"color\": \"red\", \"symbol\": \"x\", \"size\": 12},\n",
    "        name=\"Cluster Centers\",\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhJNrP6Rx56-"
   },
   "source": [
    "To solve this problem, we can execute multiple $K-means$ with different number of groups and choose the one which meets a certain criteria. There are several criteria that allow us to measure \"how well\" the clusters have achieved. The two most famous criteria are the $Elbow$  and the $Silhouette$ methods. For more information, see the following link:\n",
    "\n",
    "https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
    "\n",
    "We use $Silhoutte$ for two reasons:\n",
    "\n",
    "* Already implemented in $sklearn$.\n",
    "* The optimum number of groups is automatically extracted.\n",
    "\n",
    "**This criteria is based on the idea that a point will belong to a group if it is very close to its centroid and very far from the another ones.**\n",
    "\n",
    "See the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAziFhjCx56-"
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "groups = np.arange(2, 11)  # 2, 3, 4, ..., 8, 9, 10\n",
    "\n",
    "for k in groups:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10).fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    scores.append(silhouette_score(X, labels, metric=\"euclidean\"))\n",
    "\n",
    "# Create a figure using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add a line trace for silhouette scores\n",
    "fig.add_trace(go.Scatter(x=groups, y=scores, mode=\"lines+markers\"))\n",
    "fig.update_layout(\n",
    "    title=\"Silhouette Scores for Different Numbers of Clusters\",\n",
    "    xaxis_title=\"Number of Clusters\",\n",
    "    yaxis_title=\"Silhouette Score\",\n",
    "    xaxis={\"tickmode\": \"linear\"},\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSDOmplhIXkk"
   },
   "source": [
    "[Check Silhouette analysis](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html) from scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSyeOGSkx56_"
   },
   "source": [
    "#### Lineal Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y33JKU8lx56_"
   },
   "source": [
    "The fundamental model assumptions of k-means (points will be closer to their own cluster center than to others) means that the algorithm will often be ineffective if the clusters have complicated geometries.\n",
    "\n",
    "In particular, the boundaries between k-means clusters will always be linear, which means that it will fail for more complicated boundaries. Consider the following data, along with the cluster labels found by the typical k-means approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ph0noo2Vx57A"
   },
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=400, factor=0.3, noise=0.05)\n",
    "fig = px.scatter(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    color=y.astype(str),\n",
    "    title=\"Circles Dataset - Instance Classes\",\n",
    "    labels={\"x\": \"Feature 1\", \"y\": \"Feature 2\", \"color\": \"Class\"},\n",
    ")\n",
    "fig.update_traces(marker={\"size\": 10})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5xqhKMlx57A"
   },
   "source": [
    "### Do the groups above have a linear separation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1MsSUAxx57B"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2).fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bR3RMhndx57B"
   },
   "outputs": [],
   "source": [
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add data points colored by their cluster labels\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\"color\": y_kmeans, \"colorscale\": \"viridis\", \"size\": 10},\n",
    "        name=\"Data Points\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add cluster centers\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=centers[:, 0],\n",
    "        y=centers[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\"color\": \"red\", \"size\": 12, \"symbol\": \"x\"},\n",
    "        name=\"Cluster Centers\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title=\"K-means Clustering Results\", xaxis_title=\"Feature 1\", yaxis_title=\"Feature 2\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D9QpjMex57C"
   },
   "source": [
    "In order to solve this, we can use a kernel transformation to project the data into a higher dimension where a linear separation is possible. We might imagine using the same trick to allow k-means to discover non-linear boundaries.\n",
    "\n",
    "One version of this kernelized k-means is implemented in Scikit-Learn within the $SpectralClustering$ estimator. It uses the graph of nearest neighbors to compute a higher-dimensional representation of the data, and then assigns labels using a k-means algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBFYl0ZAx57C"
   },
   "outputs": [],
   "source": [
    "model = SpectralClustering(n_clusters=2, affinity=\"nearest_neighbors\", assign_labels=\"kmeans\")\n",
    "labels = model.fit_predict(X)\n",
    "\n",
    "# Create a figure using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add data points colored by their cluster labels\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\"color\": labels, \"colorscale\": \"viridis\", \"size\": 10},\n",
    "        name=\"Data Points\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title=\"Spectral Clustering Results\", xaxis_title=\"Feature 1\", yaxis_title=\"Feature 2\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YElB5Ghzx57C"
   },
   "source": [
    "**In real cases**, it's hard to check if your clusters have a linear separation because of the number of dimensions. The approach will be to try different models and see how it works based on your requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0YKDe1zx57D"
   },
   "source": [
    "## Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2a8NUYhx57D"
   },
   "source": [
    "The standard k-means algorithm isn't directly applicable to categorical data, for various reasons. The sample space for categorical data is discrete, and doesn't have a natural origin. A Euclidean distance function on such a space isn't really meaningful.\n",
    "\n",
    "There's a variation of k-means known as k-modes, introduced in this paper by Zhexue Huang, which is suitable for categorical data:\n",
    "\n",
    "http://www.cs.ust.hk/~qyang/Teaching/537/Papers/huang98extensions.pdf\n",
    "\n",
    "An example in python:\n",
    "\n",
    "https://towardsdatascience.com/the-k-prototype-as-clustering-algorithm-for-mixed-data-type-categorical-and-numerical-fe7c50538ebb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
