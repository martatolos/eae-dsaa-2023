{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/martatolos/eae-dsaa-2025/blob/main/decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0If0kpZxb-7"
   },
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Goal of the session:\n",
    ">\n",
    "> - At the end of this activity, you will understand the basics of decision tress, how they work internally and how are they used in random forests. Also we will see how models are evaluated and see how can we try to explain the model predictions.\n",
    ">\n",
    "> Scope of the session\n",
    ">\n",
    "> - Prepare a dataset for training a decision tree model.\n",
    "> - Analyze the dataset and see how to split it into training and test sets.\n",
    "> - Train a decision tree model using the `sklearn` library and observe how the trained model inference works.\n",
    "> - Train a random forest model.\n",
    "> - See how model performance can be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "- ``dtreeviz`` 2.2.2\n",
    "- ``ipython``\n",
    "- ``nbformat``\n",
    "- ``numpy`` 2.0.2\n",
    "- ``pathlib``\n",
    "- ``plotly`` 5.24.1\n",
    "- ``pydotplus`` 2.0.2\n",
    "- ``scikit-learn`` 1.6.1\n",
    "- ``seaborn`` 0.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VB8Jgi0exb-8"
   },
   "outputs": [],
   "source": [
    "%pip install dtreeviz==2.2.2 ipython nbformat numpy==2.0.2 pathlib plotly==5.24.1 pydotplus==2.0.2 scikit-learn==1.6.1 \\\n",
    "    seaborn==0.13.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "\n",
    "import dtreeviz\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzqAj4hJxb_A"
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris(as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4OlP-ipNPPt"
   },
   "source": [
    "## 2. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECLbw_2oNQzr"
   },
   "outputs": [],
   "source": [
    "iris.frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``iris`` is not a dataset, but a collection of datasets (``data``, ``target`` and both merged together as ``frame``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmEXImDCxb_C"
   },
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqQnBUc4xb_M"
   },
   "source": [
    "If you notice the name of the variables above, you'll see the X's and y's. Y is that?\n",
    "\n",
    "In math notation, we define the dataset X as the array of feature vectors, each vector representing the description of a flower in our dataset, each feature representing an aspect of a flower). With all these values in X, we want to infer an approximate function $\\hat{y}$, using the labeled dataset in variable `y` to better **fit** the data in `X`.\n",
    "\n",
    "Or $\\hat{y} = f(X)$\n",
    "\n",
    "Here we also use 'y' as the name of the variable that holds the target variable (species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKR92gneNdB7"
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Nh8DGvjxb_C"
   },
   "outputs": [],
   "source": [
    "y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNSxhHBrxb_C"
   },
   "outputs": [],
   "source": [
    "y.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all the values for the target occur in similar amounts, we can say that the dataset is balanced. If not, we can say that the dataset is unbalanced, which can be an issue.\n",
    "\n",
    "Do you think this dataset is balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3V3WQCOLxb_D"
   },
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xd34Iu-1xb_D"
   },
   "outputs": [],
   "source": [
    "iris_df = X.copy()\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F95x6ktmxb_F"
   },
   "outputs": [],
   "source": [
    "species_dict = {0: \"setosa\", 1: \"versicolor\", 2: \"virginica\"}\n",
    "iris_df[\"species\"] = y.map(species_dict)\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUvPDI_kxb_G"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOL0JPI6xb_H"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(iris_df, hue=\"species\", plot_kws={\"alpha\": 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNfxczbsR9WY"
   },
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the dataset into train and test datasets\n",
    "\n",
    "We are going to create a decision tree with a partition between train and test of the data. Look at the variable *random_state* that is applied in the `train_test_split` function. By changing this variable, the random distribution between the train and test data will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RbT1ozbxb_J"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=70, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random state or seed is a number that is used to initialize the random number generator. It is used to ensure that the random numbers generated are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RV-FHmzkxb_K"
   },
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHa92jsNxb_K"
   },
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkyB3x8gxb_K"
   },
   "source": [
    "### Why dividing the dataset is important?\n",
    "\n",
    "You want your model to generalize well, so sampling is important and you need to avoid data leakage, i.e. using the training dataset as the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional resources:\n",
    "- [K Fold Cross Validation](https://www.datacamp.com/tutorial/k-fold-cross-validation)\n",
    "- [K Fold Cross Validation in Scikit-Learn](https://www.cloudzilla.ai/dev-education/how-to-implement-k-fold-cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Decision Tree Classifier\n",
    "\n",
    "In decision trees we're creating structures like:\n",
    "\n",
    "![](https://miro.com/blog/wp-content/uploads/2021/12/decision_tree_business_analysis.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3npPOcTLxb_L"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model\n",
    "# X_train contains the feature vectors for examples of flowers\n",
    "# y_train contains the classes of these flowers\n",
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXjLjn5sxb_L"
   },
   "source": [
    "### Exporting model to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8mZZ6bhxb_M"
   },
   "source": [
    "This can be used to create a flower classifier application, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgAxLynkxb_M"
   },
   "outputs": [],
   "source": [
    "# Saves file (serialized dtree object)\n",
    "with Path(\"dtree_iris.pkl\").open(\"wb\") as f:  # wb stands for Write Binary\n",
    "    pickle.dump(dtree, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!Warning]\n",
    "> Some vulnerabilities have been found in the pickle file format. It can be useful for quickly testing and prototyping, but it is not recommended for production use, where other formats are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2jKDTy8xb_M"
   },
   "outputs": [],
   "source": [
    "# Just showing the file was actually created\n",
    "!ls | grep pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0t2P3e2ixb_N"
   },
   "source": [
    "## 4. Analyzing and evaluating the model\n",
    "\n",
    "### Visualizing resulting decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YWVpyY1xb_N"
   },
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(\n",
    "    dtree,\n",
    "    out_file=dot_data,\n",
    "    feature_names=iris.feature_names,\n",
    "    class_names=iris.target_names,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=True,\n",
    ")\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQC01F0Lxb_I"
   },
   "source": [
    "**Decision Trees** or CART (Classification and Regression Trees):\n",
    "\n",
    "This method is based on the order of the data. It is, therefore, an iterative method in which each step will try to reduce the *impurity* of the data based on the variable to be predicted. Digging a bit:\n",
    "\n",
    "1. We start with all our data in the same bag, therefore I have all the different categories mixed together.\n",
    "By categories we mean the class, the target variable, i.e. what we're trying to predict in a classification task.\n",
    "\n",
    "    1. The impurity of this data bag is modeled using the **Gini impurity index** (not to be confused with the Gini coefficient).\n",
    "    1. Simplifying it, it could be seen as: If we introduce a new observation into our bag, whose response variable has been chosen based on the distribution of the different categories, what is the probability of being wrong if I try to classify it?\n",
    "    1. In other terms: if I have a sack with two pears and two oranges, the impurity is 50%, since a new variable could be with the same probability either pear or orange.\n",
    "\n",
    "2. Then, as we want to reduce this impurity, we will distribute the observations in different bags in each iteration based on the characteristics of our variables until we obtain groups where there is no possible error: Either everything is pears or everything is oranges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8R_bARbxb_N"
   },
   "source": [
    "We can see that there is a variable that strongly marks a first cut, so that already in the first split we obtain a partition without impurity. It is a model capable of obtaining a very accurate division of the data in very few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICVbrvmkxb_O"
   },
   "outputs": [],
   "source": [
    "viz = dtreeviz.model(\n",
    "    dtree,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    target_name=\"Species\",\n",
    "    feature_names=iris.feature_names,\n",
    "    class_names=list(iris.target_names),\n",
    ")\n",
    "\n",
    "viz.view(fontname=\"DejaVu Sans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJH-TYzrxb_O"
   },
   "source": [
    "If we now repeat this process with a different distribution between train and test, which implies that the model has been trained with other data, by changing `random_state` to other value ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttiA5CQnxb_P"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=60, stratify=y)\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "viz = dtreeviz.model(\n",
    "    dtree,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    target_name=\"Species\",\n",
    "    feature_names=iris.feature_names,\n",
    "    class_names=list(iris.target_names),\n",
    ")\n",
    "\n",
    "viz.view(fontname=\"DejaVu Sans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YL5ZTFiqxb_P"
   },
   "source": [
    "### Some words about overfitting and underfitting\n",
    "\n",
    "Sometimes you train a ML algorithm on the labeled dataset you have, but when faced with new unseen data the model starts performing bad, and therefore making wrong classifications. This happens because the model hasn't generalized the data patterns correctly. It was only replicating the data distribution in a highly specialized or generic way.\n",
    "\n",
    "![](https://www.mathworks.com/discovery/overfitting/_jcr_content/mainParsys/image.adapt.full.medium.svg/1705396624275.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_Ddt_3kxb_P"
   },
   "source": [
    "### Train-validation-test split\n",
    "\n",
    "One way to try to cope with overfitting is doing a more complex strategy for dividing the dataset into:\n",
    "\n",
    "1. Train datased: used to train the model\n",
    "1. Validation (or development) dataset: used to optimize our model parameters to achieve best accuracy possible\n",
    "1. Test dataset: unseen data separated to test our model with possibly different patterns not found in previous datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9CLIadCxb_Q"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.10, random_state=42, stratify=y_train)\n",
    "\n",
    "# F-strings https://realpython.com/python-f-strings/\n",
    "print(f\"Training size: {len(X_train)}, Validation size: {len(X_dev)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrOuItGhxb_Q"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model\n",
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5D3zaEC_xb_Q"
   },
   "outputs": [],
   "source": [
    "viz = dtreeviz.model(\n",
    "    dtree,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    target_name=\"Species\",\n",
    "    feature_names=iris.feature_names,\n",
    "    class_names=list(iris.target_names),\n",
    ")\n",
    "\n",
    "viz.view(fontname=\"DejaVu Sans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mX7MiTlUxb_R"
   },
   "source": [
    "### Checking accuracy with the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8tabGgBxb_R"
   },
   "outputs": [],
   "source": [
    "y_dev_predicted = dtree.predict(X_dev)\n",
    "y_dev_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy with validation dataset {accuracy_score(y_dev, y_dev_predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLAc2Jezxb_R"
   },
   "source": [
    "At this point we can do multiple runs of train/validation checks to improve the accuracy score..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKs-ZHx3xb_R"
   },
   "source": [
    "### Inferring over new unforeseen data (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhORvkeuxb_S"
   },
   "outputs": [],
   "source": [
    "# The same but with X_test, y_test\n",
    "y_test_predicted = dtree.predict(X_test)\n",
    "y_test_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy with test dataset {accuracy_score(y_test, y_test_predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7BGdsYcxb_S"
   },
   "source": [
    "## 5. Decision trees for regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlS1tfHExb_T"
   },
   "source": [
    "### Diabetes dataset\n",
    "\n",
    "Target is a quantitative measure of disease progression one year after baseline\n",
    "\n",
    "![](https://explained.ai/decision-tree-viz/images/samples/diabetes-TD-3-X.svg)\n",
    "\n",
    "In the above example we can see Decision Trees create **LINEAR** decision boundaries for its variables.\n",
    "This is amazing to generate (visual) explanations for your resulting model BUT...\n",
    "\n",
    "What would happen if the feature vector distribution is like in the following image:\n",
    "\n",
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20200605170732/linearsep.png)\n",
    "\n",
    "In this case we probably need Support Vector Machines (SVM), which are able to divide the feature space into curved decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQ3rgCiSxb_T"
   },
   "source": [
    "### Additional reading\n",
    "\n",
    "- [A visual introduction to machine learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "- [Understanding Decision Trees with Python](https://www.datacamp.com/tutorial/decision-tree-classification-python)\n",
    "- [4 Ways to Visualize Individual Decision Trees in a Random Forest](https://towardsdatascience.com/4-ways-to-visualize-individual-decision-trees-in-a-random-forest-7a9beda1d1b7)\n",
    "- [How to Visualize a Random Forest with Fitted Parameters?](https://analyticsindiamag.com/how-to-visualize-a-random-forest-with-fitted-parameters/)\n",
    "- [Understanding Random forest better through visualizations](https://garg-mohit851.medium.com/random-forest-visualization-3f76cdf6456f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY64pWXfxb_U"
   },
   "source": [
    "## 6. Improving the Model\n",
    "\n",
    "There are two very useful techniques that allow us to create new models:\n",
    "\n",
    "* **Bootstrap**: It is based on choosing subsamples of our data in a uniform way and with repetition, thus creating multiple smaller samples that share the distributions of the original sample.\n",
    "\n",
    "* **Bagging**: Generate a bootstrap of size $n$, train a model on that subsample and repeat the process $m$ times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GW8zY4fjxb_U"
   },
   "source": [
    "## 7. Random Forests\n",
    "\n",
    "Now let's apply an *ensemble* method such as *Random Forest*, based on the results of multiple decision trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "absn-PZLxb_U"
   },
   "source": [
    "There are other types of combining classifiers (or ensemble methods), such as mean average, or the product rule, using a priory probability distributions and the confidence of the classification of each classifier.\n",
    "\n",
    "Additional reading:\n",
    "\n",
    "- [Scikit Learn ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0DfdW7Axb_U"
   },
   "source": [
    "### Training a Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lm0Wiw4axb_V"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=100, stratify=y)\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)  # parallelize the execution\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhADPZftxb_V"
   },
   "source": [
    "Let's analyze the code ... ([RFs in Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6B26aFYxb_V"
   },
   "outputs": [],
   "source": [
    "help(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``help`` function is very useful to understand the parameters of a function. You can also use the ``?`` operator in Jupyter notebooks to get help on a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfGyWbKrxb_V"
   },
   "source": [
    "### Real species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYm8_6PExb_V"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8oy5Sr6xb_W"
   },
   "source": [
    "### Predicting the class of test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gf6lrbPwxb_W"
   },
   "outputs": [],
   "source": [
    "predicted = rf.predict(X_test)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccH2FXOYxb_W"
   },
   "source": [
    "### Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sESEo-xPxb_X"
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kt1SLP2Nxb_X"
   },
   "outputs": [],
   "source": [
    "help(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlRX-1ouxb_X"
   },
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IyAbHtyxb_X"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_jobs=4)\n",
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTU6nWANxb_Y"
   },
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmXSL6bQxb_Y"
   },
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjmWAx9Gxb_Y"
   },
   "outputs": [],
   "source": [
    "indexes = np.argsort(importances)[::-1]\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0t3ddweQxb_Y"
   },
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQDole2Nxb_Y"
   },
   "outputs": [],
   "source": [
    "# Map importances to species_names\n",
    "names = [X.columns[i] for i in indexes]\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z674qMijxb_Z"
   },
   "outputs": [],
   "source": [
    "# Prepare a barplot with plotly\n",
    "fig = px.bar(x=names, y=importances[indexes], labels={\"x\": \"Features\", \"y\": \"Importance\"}, title=\"Feature Importance\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEscIEHBxb_Z"
   },
   "source": [
    "In the code of the upper cell we see not only how we can apply a model, but that it contains the importance of the different *features* or predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UghMh4TExb_Z"
   },
   "source": [
    "### Predict() function and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtnxETsnxb_Z"
   },
   "outputs": [],
   "source": [
    "rf_preds = rf.predict(X_test)\n",
    "rf_conf_mat = confusion_matrix(y_test, rf_preds)\n",
    "rf_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHnjj_Dwxb_Z"
   },
   "outputs": [],
   "source": [
    "help(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHIJUdmLxb_a"
   },
   "source": [
    "You can get the accuracy score with data from confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuKV7IWSxb_a"
   },
   "outputs": [],
   "source": [
    "# Convert to numpy\n",
    "np_mat = np.asarray(rf_conf_mat)\n",
    "\n",
    "acc = sum(np.diagonal(np_mat)) / np_mat.sum()\n",
    "print(f\"My accuracy is: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNyEDXYCxb_a"
   },
   "source": [
    "### Confusion matrix visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JN3hkC7dxb_a"
   },
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=rf_conf_mat, display_labels=iris.target_names)\n",
    "disp.plot()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
