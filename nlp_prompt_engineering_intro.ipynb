{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/martatolos/eae-dsaa-2025/blob/main/nlp_prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Goal of the session:\n",
    ">\n",
    "> - At the end of this activity, you will be able to write your own prompts efficiently and strategies to build them.\n",
    ">\n",
    ">Scope of the session\n",
    ">\n",
    "> - We will walk through the basic concepts to learn the basics on prompt engineering.\n",
    "> - We will use an instruction tuned OpenAI LLM\n",
    "> - How to write clear and specific prompts\n",
    ">   - Use of delimiters\n",
    ">   - Structured output\n",
    ">   - Conditions check\n",
    ">   - Few-shot prompting\n",
    "> - How to let the model simulate thinking process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouxNaJDHvt00"
   },
   "source": [
    "###  1. Base LLMs vs Instruction Tuned LLMs\n",
    "\n",
    "First, let's understand the difference between a base LLM and an instruction-tuned LLM.\n",
    "\n",
    "#### Base LLM\n",
    "\n",
    "Predicts next word, based on text training data.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Prompt\n",
    "```\n",
    "Once upon a time, there was a unicorn\n",
    "```\n",
    "\n",
    "Answer\n",
    "```\n",
    "that lived in a magical forest with all her unicorn friends\n",
    "```\n",
    "\n",
    "Prompt\n",
    "```\n",
    "What is the capital of France?\n",
    "```\n",
    "\n",
    "Answer\n",
    "```\n",
    "What is France's largest city?\n",
    "What is France's popilation?\n",
    "What is the currency of France?\n",
    "```\n",
    "\n",
    "#### Instruction Tuned LLM\n",
    "\n",
    "Tries to follow instructions, as it is fine-tuned on instructions and good examples at following those instructions.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Prompt\n",
    "```\n",
    "What is the capital of France?\n",
    "```\n",
    "\n",
    "Answer\n",
    "```\n",
    "The capital of France is Paris.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prompt Engineering Basic Understandings\n",
    "\n",
    "#### What is Prompt Engineering\n",
    "\n",
    "Prompt engineering refers to the process of crafting effective prompts or input queries to elicit desired responses from the model. This process involves understanding the capabilities and limitations of the LLM and designing prompts that guide it towards generating the desired outputs.\n",
    "\n",
    "#### What is a prompt\n",
    "\n",
    "In the context of Generative AI, a prompt is a specific input provided to an AI model to generate desired outputs, such as text, images, or even music. The prompt serves as the starting point or guiding instruction for the model, influencing the content and style of the generated output.\n",
    "\n",
    "Example of prompt:\n",
    "```\n",
    "Could you recommend 5 lesser-known spots in Barcelona?\n",
    "```\n",
    "\n",
    "### Prompt engineering (some) techniques\n",
    "\n",
    "Prompt engineering can include various techniques such as:\n",
    "\n",
    "1. **Prompt formulation**: Crafting the wording and structure of the prompt to provide the necessary context and constraints for the model to generate the desired response.\n",
    "2. **Prompt tuning**: Iteratively adjusting the prompt based on the model's responses to improve the quality and relevance of the generated outputs.\n",
    "3. **Prompt augmentation**: Adding additional information or cues to the prompt to guide the model towards specific types of responses or to encourage more diverse outputs.\n",
    "4. **Prompt analysis**: Analyzing the effectiveness of different prompts in influencing the model's behavior and using this insight to refine future prompt designs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3. Setup"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies\n",
    "\n",
    "- ``ipython``\n",
    "- ``openai`` 1.75.0\n",
    "- ``python-dotenv``"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17802,
     "status": "ok",
     "timestamp": 1715187817389,
     "user": {
      "displayName": "ML LS",
      "userId": "06944350798110301808"
     },
     "user_tz": -120
    },
    "id": "JWDtJYtT68T2",
    "outputId": "25556742-9bf8-4d61-c1bb-0375040e4dff"
   },
   "source": [
    "%pip install ipython openai==1.75.0 python-dotenv"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import dotenv\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Key\n",
    "\n",
    "Add your OpenAI API key in the cell below or create a `.env` file in the same directory as this notebook with the following content:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "```\n",
    "\n",
    "> [!Warning]\n",
    "> Make sure you do not save or commit the file without removing your API key. If that happens, reset the key so that it is not compromised."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 1062,
     "status": "ok",
     "timestamp": 1715189665515,
     "user": {
      "displayName": "ML LS",
      "userId": "06944350798110301808"
     },
     "user_tz": -120
    },
    "id": "3UCWomYpEIgv"
   },
   "source": [
    "open_ai_key = None  # Add your OpenAI API key here\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", open_ai_key)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDur0NLgHNAs"
   },
   "source": [
    "### Helper functions\n",
    "\n",
    "Next, we will create a function that we will use in today's activity. We will use [chat completions endpoint](https://platform.openai.com/docs/guides/text-generation/chat-completions-api). Also most LLMs use markdown format to in their output. We will use ``render_markdown`` function to show the outputs we get in a more readable format."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1715189727541,
     "user": {
      "displayName": "ML LS",
      "userId": "06944350798110301808"
     },
     "user_tz": -120
    },
    "id": "xDPrAFCoDiyY"
   },
   "source": [
    "def get_completion(prompt: str, model_name: str = \"gpt-4.1-nano\") -> str:\n",
    "    \"\"\"Get the completion from OpenAI API.\n",
    "\n",
    "    :param prompt: Prompt to be sent to the model.\n",
    "    :param model_name: Name of the model which will be used.\n",
    "        Check https://platform.openai.com/docs/models to get an updated list.\n",
    "        Defaults to \"gpt-4o-mini\"\n",
    "    :return: Completion from the model.\n",
    "    \"\"\"\n",
    "    return OpenAI().responses.create(model=model_name, input=prompt).output[0].content[0].text\n",
    "\n",
    "\n",
    "def render_markdown(text: str) -> None:\n",
    "    \"\"\"Render the text as markdown.\n",
    "\n",
    "    :param text: Text to be rendered.\n",
    "    \"\"\"\n",
    "    display(Markdown(text))\n",
    "\n",
    "\n",
    "def show_completion(prompt: str, model_name: str = \"gpt-4.1-nano\") -> None:\n",
    "    \"\"\"Get the completion from OpenAI API and render it as markdown.\n",
    "\n",
    "    :param prompt: Prompt to be sent to the model.\n",
    "    :param model_name: Name of the model which will be used.\n",
    "        Check https://platform.openai.com/docs/models to get an updated list.\n",
    "        Defaults to \"gpt-4o-mini\"\n",
    "    \"\"\"\n",
    "    completion = get_completion(prompt, model_name)\n",
    "    render_markdown(completion)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "executionInfo": {
     "elapsed": 986,
     "status": "ok",
     "timestamp": 1715189674482,
     "user": {
      "displayName": "ML LS",
      "userId": "06944350798110301808"
     },
     "user_tz": -120
    },
    "id": "br2ZAugp7iOK",
    "outputId": "351f19ca-b8c9-4f3e-97f5-987eb3c94431"
   },
   "source": [
    "prompt = \"Tell me a joke\"\n",
    "show_completion(prompt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLEAadwCjc-I"
   },
   "source": [
    "## 4. Prompting Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to write clear and specific prompts\n",
    "\n",
    "- If our prompt is vague, we are giving freedom to the model to:\n",
    "  - Decide whether there are any information gaps in our instruction.\n",
    "  - Fill the information gaps in our instruction, if the model detects that the information in the prompt is not complete.\n",
    "- Easily, this scenario can lead to poor generated answers or answers with  non-factual content.\n",
    "- To avoid or mitigate ambiguity in the instruction, our prompt needs to be clear and specific to help the model to focus on a precise task to solve.\n",
    "- Clarity and specificity does not have anything to do with the length of the prompt (i.e. short prompts do not necessary are clear and specific)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a vague rather short prompt:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "executionInfo": {
     "elapsed": 11284,
     "status": "ok",
     "timestamp": 1715189743544,
     "user": {
      "displayName": "ML LS",
      "userId": "06944350798110301808"
     },
     "user_tz": -120
    },
    "id": "yiSfpWl6-_hK",
    "outputId": "35c58019-7a20-4fdf-d211-093fce69087a"
   },
   "source": [
    "prompt = \"Write a blog post about Artificial Intelligence\"\n",
    "show_completion(prompt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2di0VMP3lPgC"
   },
   "source": [
    "Example of a clear and specific prompt"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "executionInfo": {
     "elapsed": 6213,
     "status": "ok",
     "timestamp": 1715189753855,
     "user": {
      "displayName": "ML LS",
      "userId": "06944350798110301808"
     },
     "user_tz": -120
    },
    "id": "jZmhsj4LlLzj",
    "outputId": "8e719b74-1ca2-43af-d3d9-0fac1188b79d"
   },
   "source": [
    "prompt = \"\"\"\n",
    "Write a 200 words long blog post about Artificial Intelligence. The post should \\\n",
    "include the three most relevant and recent advancements in Artificial Intelligence \\\n",
    "using text to image model techniques. The reader is a general audience.\n",
    "\"\"\"\n",
    "\n",
    "show_completion(prompt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pST09BeQrLrW"
   },
   "source": [
    "### Prompting Techniques\n",
    "\n",
    "There are some techniques for writing clear and specific prompts. We will explore the following ones:\n",
    "\n",
    "1. Use delimiters to mark the boundaries of data sources\n",
    "2. Instruct the model to generate a structured output\n",
    "3. Check whether the conditions are met\n",
    "4. Insert examples to show the model to solve the task (few-shot prompting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Use delimiters to mark the boundaries of data sources/task\n",
    "\n",
    "Examples\n",
    "- Double quotes: \"\"\n",
    "- Triple backticks: ``\n",
    "- Triple dashes: ---\n",
    "- Angle brackets: <>\n",
    "- XML tags: <tag> </tag>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "executionInfo": {
     "elapsed": 1815,
     "status": "ok",
     "timestamp": 1715190361795,
     "user": {
      "displayName": "ML LS",
      "userId": "06944350798110301808"
     },
     "user_tz": -120
    },
    "id": "QPCSjcUBmR0n",
    "outputId": "13b59d1d-7188-4041-e9ae-a13a8b000c14"
   },
   "source": [
    "# \"Stop children using smartphones until they are 13, says French report\", The Guardian, 30th of April 2024 (https://www.theguardian.com/world/2024/apr/30/stop-children-using-smartphones-until-they-are-13-say-french-experts-in-report)\n",
    "text = \"\"\"\n",
    "Children should not be allowed to use smartphones until they are 13 and should be banned from accessing \\\n",
    "conventional social media such as TikTok, Instagram and Snapchat until they are 18, according to a report \\\n",
    "by experts commissioned by Emmanuel Macron.\n",
    "\n",
    "The French president had asked scientists and experts to suggest screen use guidelines for children with \\\n",
    "a view to France taking unprecedented steps on limiting their exposure. It was unclear how the government \\\n",
    "might now proceed after the report's publication. Macron said in January: “There might be bans, there might \\\n",
    "be restrictions.”\n",
    "\n",
    "The hard-hitting report said children needed to be protected from the tech industry's profit-driven “strategy \\\n",
    "of capturing children's attention, using all forms of cognitive bias to shut children away on their screens, \\\n",
    "control them, re-engage them and monetise them”.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks into a single sentence\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "show_completion(prompt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJswWNditOPw"
   },
   "source": [
    "#### Instruct the model to generate a structured output\n",
    "\n",
    "Examples\n",
    "HMTL\n",
    "JSON"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "executionInfo": {
     "elapsed": 4581,
     "status": "ok",
     "timestamp": 1714562352501,
     "user": {
      "displayName": "spam lloberes",
      "userId": "06944350798110301808"
     },
     "user_tz": -120
    },
    "id": "FkkIQuxXtFLy",
    "outputId": "3206ce2d-d62a-46ef-d66d-2b620eb183eb"
   },
   "source": [
    "prompt = \"\"\"\n",
    "Provide a list of five movies with the corresponding director and genres.\n",
    "\n",
    "Your answer should be a well formatted JSON following the keys: movie_id, title, director, genre.\n",
    "\n",
    "Please just provide the JSON without any additional text or explanation. Also do not encase it in triple backticks.\n",
    "\"\"\"\n",
    "\n",
    "json_response = get_completion(prompt)\n",
    "\n",
    "render_markdown(f\"```json\\n{json_response}\\n```\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!Note]\n",
    "> There is not guarantee that the schema of the answer is going to be always correct.\n",
    "\n",
    "If the model provides a properly structured output, we can use it to parse the answer and use it in our application. If the model does not provide a properly structured output, we can use the answer as a text and parse it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "parsed_response = json.loads(json_response)\n",
    "\n",
    "movie_info_template = \"\"\"\n",
    "-----------------\n",
    "MOVIE ID: {movie_id}\n",
    "TITLE: {title}\n",
    "DIRECTOR: {director}\n",
    "GENRE: {genre}\n",
    "-----------------\n",
    "\"\"\"\n",
    "\n",
    "for response in parsed_response:\n",
    "    movie_info = movie_info_template.format(\n",
    "        movie_id=response[\"movie_id\"], title=response[\"title\"], director=response[\"director\"], genre=response[\"genre\"]\n",
    "    )\n",
    "    print(movie_info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67XoFtv5vVOK"
   },
   "source": [
    "#### Check whether the conditions are met"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 2678,
     "status": "ok",
     "timestamp": 1714562356730,
     "user": {
      "displayName": "spam lloberes",
      "userId": "06944350798110301808"
     },
     "user_tz": -120
    },
    "id": "ydw-D1xsu_rI",
    "outputId": "505fb327-1c2f-4617-a361-9fd20fdeeefd"
   },
   "source": [
    "text_1 = \"\"\"\n",
    "<text>To make Eggs Benedict, first start by poaching eggs in simmering water with a dash of vinegar to \\\n",
    "help set the whites. Next, while the eggs cook, toast English muffins until golden brown. Third, \\\n",
    "warm slices of Canadian bacon in a skillet. Once all the ingredients are ready, assemble the dish \\\n",
    "by placing the toasted muffin halves on a plate, topping each half with a slice of Canadian bacon,\n",
    "followed by a poached egg. Lastly, drizzle hollandaise sauce over the eggs and garnish with chopped \\\n",
    "parsley or paprika for a classic touch. Serve immediately and enjoy the rich, indulgent flavors of Eggs Benedict!</text>\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are given a text delimited by tags <text> </text>. If it contains a sequence of instructions, \\\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1) Place instruction 1 here\n",
    "Step 2) Place instruction 2 here\n",
    "Step 3) Place instruction 3 here\n",
    "...\n",
    "Step N) Place instruction N here\n",
    "\n",
    "If the text does not contain a sequence of instructions, then answer 'No instructions provided'.\n",
    "\n",
    "<text>{text_1}</text>\n",
    "\"\"\"\n",
    "\n",
    "show_completion(prompt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5maXOCDpzeAn"
   },
   "source": [
    "Let's see what happens with other kinds of text."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 756,
     "status": "ok",
     "timestamp": 1714562359932,
     "user": {
      "displayName": "spam lloberes",
      "userId": "06944350798110301808"
     },
     "user_tz": -120
    },
    "id": "MUHFn5aUxsoL",
    "outputId": "6bbb4d37-d689-401a-95d1-931192c72541"
   },
   "source": [
    "text_2 = \"\"\"\n",
    "I soon learned to know this flower better. On the little prince's planet the flowers had \\\n",
    "always been very simple. They had only one ring of petals; they took up no room at all; they \\\n",
    "were a trouble to nobody. One morning they would appear in the grass, and by night they would \\\n",
    "have faded peacefully away. But one day, from a seed blown from no one knew where, a new flower \\\n",
    "had come up; and the little prince had watched very closely over this small sprout which was not \\\n",
    "like any other small sprouts on his planet. It might, you see, have been a new kind of baobab.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are given a text delimited by tags <text> </text>. If it contains a sequence of instructions, \\\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1) Place instruction 1 here\n",
    "Step 2) Place instruction 2 here\n",
    "Step 3) Place instruction 3 here\n",
    "...\n",
    "Step N) Place instruction N here\n",
    "\n",
    "If the text does not contain a sequence of instructions, then answer 'No instructions provided'. \\\n",
    "Please, follow the examples delimited by <example> tags.\n",
    "\n",
    "<text>{text_2}</text>\n",
    "<example>type our examples</example>\n",
    "\"\"\"\n",
    "\n",
    "show_completion(prompt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B__4J4uc3voc"
   },
   "source": [
    "#### Insert examples to show the model to solve the task (few-shot prompting)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1685,
     "status": "ok",
     "timestamp": 1714578173848,
     "user": {
      "displayName": "spam lloberes",
      "userId": "06944350798110301808"
     },
     "user_tz": -120
    },
    "id": "MVEHbqOuzk4K",
    "outputId": "a7b5d430-ba81-43d1-8b27-9a96cc92a4b4"
   },
   "source": [
    "prompt = \"\"\"\n",
    "The word \"sunsapphire\" refers to the sun's warm glow and the brilliant blue hues of the summer sky at sunset.\n",
    "\n",
    "To \"glintle\" means to emit a soft, subtle sparkle or shimmer, typically associated with delicate or understated \\\n",
    "beauty. An example of a sentence that uses the word \"glintle\" is:\n",
    "\"\"\"\n",
    "\n",
    "get_completion(prompt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Srq7VLMZ189L"
   },
   "source": [
    "###  How to let the model simulate thinking process\n",
    "\n",
    "- Sometimes the model makes reasoning errors, which leads to a wrong conclusion.\n",
    "- Other times the task given to the model is too complex to solve, so the model may generate an incorrect answer.\n",
    "- A way to mitigate these errors is to structure the prompt in a series or a chain of actions, so that the model is going to spend more computational power to solve the task.\n",
    "\n",
    "There are some techniques for giving the model time to think. We will explore the following ones:\n",
    "\n",
    "- Define the steps to complete a task\n",
    "- Let the model find the solution on its own before rushing to a conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the steps to complete a task\n",
    "\n",
    "```\n",
    "Step 1: ...\n",
    "Step 2: ...\n",
    "...\n",
    "Step N: ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "executionInfo": {
     "elapsed": 3722,
     "status": "ok",
     "timestamp": 1714580640862,
     "user": {
      "displayName": "spam lloberes",
      "userId": "06944350798110301808"
     },
     "user_tz": -120
    },
    "id": "DrD4hbpK4eU4",
    "outputId": "4ba05d02-77a7-4801-d5d5-b50f6a61dd54"
   },
   "source": [
    "text = \"\"\"\n",
    "On a Friday morning, Petra and Oliver, siblings bound by a bond stronger than blood, embarked on \\\n",
    "their usual journey to school. As they strolled through the bustling streets, the crisp autumn air \\\n",
    "danced around them, carrying the promise of the weekend ahead. Petra, with her curly hair bouncing \\\n",
    "in rhythm with her steps, chattered excitedly about the upcoming school project. Oliver, the quieter \\\n",
    "of the two, listened intently, his mind already drifting to the adventures awaiting them. Together, \\\n",
    "they navigated the cityscape, their laughter harmonizing with the hum of the waking city, painting \\\n",
    "the dawn with hues of warmth and anticipation.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Perform the following actions:\n",
    "1. Summarize the text delimited by triple backticks in 1 sentence.\n",
    "2. Translate the summary into Spanish.\n",
    "3. List each person name in the summary.\n",
    "4. Output a json object that contains the following keys: summary, num_names (encased in triple backticks)\n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "show_completion(prompt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqDBgvwA-gfL"
   },
   "source": [
    "Let's ask the model to structure and format the answer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 3784,
     "status": "ok",
     "timestamp": 1714581120336,
     "user": {
      "displayName": "spam lloberes",
      "userId": "06944350798110301808"
     },
     "user_tz": -120
    },
    "id": "F7sDFCEx6x0l",
    "outputId": "b9cb9e44-c18f-4a10-ad99-4ceeb3f9c061"
   },
   "source": [
    "prompt_2 = f\"\"\"\n",
    "Perform the following actions:\n",
    "1. Summarize the text delimited by triple backticks in 1 sentence.\n",
    "2. Translate the summary into Spanish.\n",
    "3. List each person name in the summary.\n",
    "4. Output a json object that contains the following keys: spanish_summary, num_names.\n",
    "\n",
    "Use the following format:\n",
    "Text: <text to summarize>\n",
    "Summary: <summary>\n",
    "Translation: <summary translation>\n",
    "Names: <list of names in summary>\n",
    "Output JSON: <json with summary and num_names encased in triple backticks>\n",
    "\n",
    "Text: ```{text}``\n",
    "\"\"\"\n",
    "show_completion(prompt_2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsA36DHGAsM2"
   },
   "source": [
    "### Let the model find the solution on its own before rushing to a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1632,
     "status": "ok",
     "timestamp": 1714581770116,
     "user": {
      "displayName": "spam lloberes",
      "userId": "06944350798110301808"
     },
     "user_tz": -120
    },
    "id": "pyN3-27G_E-f",
    "outputId": "f15701db-977c-4128-c632-8cc3fc0b3768"
   },
   "source": [
    "prompt = \"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "\n",
    "Question:\n",
    "I'm building a solar power installation and I need help working out the financials.\n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost me a flat $100k per year, and \\\n",
    "an additional $10 / square foot\n",
    "What is the total cost for the first year of operations as a function of the number of square feet.\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "\"\"\"\n",
    "show_completion(prompt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCuXrpqFBTpP"
   },
   "source": [
    "Note: The student solution is incorrect. Maintenance cost should be calculated as \"100,000 + 10x\" because it is $10 / square foot.\n",
    "\n",
    "It is possible to make the model generate the correct answer by giving the chance to find the solution on its own first."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "executionInfo": {
     "elapsed": 5319,
     "status": "ok",
     "timestamp": 1714582560616,
     "user": {
      "displayName": "spam lloberes",
      "userId": "06944350798110301808"
     },
     "user_tz": -120
    },
    "id": "JoKg6kqTBOyI",
    "outputId": "36783a1e-da76-43c2-865a-49c6eb674a8a"
   },
   "source": [
    "prompt = \"\"\"\n",
    "Your task is to determine if the student's solution is correct or not.\n",
    "To solve the problem do the following:\n",
    "- First, work out your own solution to the problem including the final total.\n",
    "- Then compare your solution to the student's solution and evaluate if the student's solution is correct or not.\n",
    "Don't decide if the student's solution is correct until you have done the problem yourself.\n",
    "\n",
    "Question:\n",
    "```\n",
    "I'm building a solar power installation and I need help working out the financials.\n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional \\\n",
    "$10 / square foot\n",
    "What is the total cost for the first year of operations as a function of the number of square feet.\n",
    "```\n",
    "Student's solution:\n",
    "```\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "```\n",
    "Actual solution:\n",
    "\n",
    "Use the following format:\n",
    "Question:\n",
    "```\n",
    "question here\n",
    "```\n",
    "Student's solution:\n",
    "```\n",
    "student's solution here\n",
    "```\n",
    "Actual solution:\n",
    "```\n",
    "steps to work out the solution and your solution here\n",
    "```\n",
    "Is the student's solution the same as actual solution \\\n",
    "just calculated:\n",
    "```\n",
    "yes or no\n",
    "```\n",
    "Student grade:\n",
    "```\n",
    "correct or incorrect\n",
    "```\n",
    "\"\"\"\n",
    "show_completion(prompt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTYdncsfFzQc"
   },
   "source": [
    "Note: LLMs are not efficient for performing calculations, or reasoning on mathematical problems.\n",
    "\n",
    "### Model limitations\n",
    "\n",
    "**Hallucinations**\n",
    "- The model can make statements that may sound true, but actually they are not true.\n",
    "- This happens because the model \"does not know where the boundaries of the knowledge learnt during training are\".\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 2805,
     "status": "ok",
     "timestamp": 1714583091255,
     "user": {
      "displayName": "spam lloberes",
      "userId": "06944350798110301808"
     },
     "user_tz": -120
    },
    "id": "EyoNfYVVDTFQ",
    "outputId": "650c31da-d5df-4c34-8352-951380fe331f"
   },
   "source": [
    "prompt = \"\"\"\n",
    "Provide a description about Coca-Cola, the newest soap dish of Fairy\n",
    "\"\"\"\n",
    "\n",
    "show_completion(prompt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuo4JAysHpCz"
   },
   "source": [
    "Strategy to mitigate hallucinations:\n",
    "- Find relevant information about the question.\n",
    "- Inject in the prompt the relevant information found.\n",
    "- Ask the model answer the question using the relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Time to practise on your own"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T3UImMilHarA"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJxulroLkp3KICE7t2hghp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
